# math-plus-summerschool
Interactive Session at the Summer School &amp; Conference: Mathematical Optimization for Machine Learning

# Interactive Session: Practical Insights into Deep Learning Optimization

## Introduction (15 minutes)
        Brief overview of deep learning.
        Importance of optimization in deep learning.
        Goals of the session.

## Foundational Concepts (20 minutes)
        Loss functions: What are they and why are they important?
        Gradient Descent: Intuitive understanding.
        Challenges in optimization: Non-convexity, saddle points, vanishing and exploding gradients.

## First-Order Optimization Algorithms (25 minutes)

    Interactive coding starts from here

    a. Stochastic Gradient Descent (SGD) (10 minutes)
    - Theory and intuition.
    - Coding task: Implement SGD and test on a toy problem.

    b. AdaGrad (5 minutes)
    - Why do we need adaptive methods?
    - Coding task: Implement AdaGrad and test on the same toy problem.

    c. Adam (10 minutes)
    - Combination of momentum and adaptive methods.
    - Coding task: Implement Adam and test on the toy problem.

## Common Pitfalls and Practical Tips (20 minutes)
        Learning rate selection and scheduling.
        Importance of weight initialization.
        The effect of batch size.
        Regularization techniques: Dropout, L2.
        Coding task: Incorporate these tips into a deep learning model and observe improvements.

## Advanced Optimization Topics (20 minutes)
        Second order methods: Newton's method, L-BFGS.
        Batch normalization: How it helps in optimization.
        Beyond first-order methods: Lookahead, RAdam.
        Coding task: Experiment with one advanced technique on a given deep learning task.

## Visualization Tools and Techniques (20 minutes)
        TensorBoard for monitoring training progress.
        Visualizing gradient flow and weight histograms.
        Understanding training dynamics via loss landscapes.
        Coding task: Incorporate TensorBoard into the earlier coding exercises and interpret visualizations.

## Case Study (30 minutes)
        Present a real-world deep learning problem (e.g., image classification).
        Participants implement various optimization techniques learned.
        Compare and contrast the performance of different optimizers on this problem.
        Interactive discussion on the results and observations.

## Wrap-up and Q&A Session (20 minutes)
        Summarize key takeaways.
        Open the floor for questions, clarifications, and further discussions.
        Provide references for deeper dives and advanced studies.

## Optional Hands-on Challenge (10 minutes)
        Give participants a small challenge problem where they can apply the optimization techniques learned.
        This serves as a test of their understanding and provides a sense of accomplishment.